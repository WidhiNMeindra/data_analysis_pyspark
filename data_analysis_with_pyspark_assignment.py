# -*- coding: utf-8 -*-
"""Data Analysis with PySpark - Assignment

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j7OPmZ5xHzSSYH_gWAT73AyoPDkZkc5c
"""

# Install pyspark
!pip install pyspark

"""# Ekstraksi Data ke dalam DataFrame"""

from pyspark.sql import SparkSession

# Inisialisasi Spark session
spark = SparkSession.builder.appName("Airline Customer Analysis").getOrCreate()

calendar_df = spark.read.csv("/content/calendar.csv", header=True, inferSchema=True)
customer_flight_activity_df = spark.read.csv("/content/customer_flight_activity.csv", header=True, inferSchema=True)
customer_loyalty_history_df = spark.read.csv("/content/customer_loyalty_history.csv", header=True, inferSchema=True)

# Memeriksa tipe data untuk setiap DataFrame
calendar_df.printSchema()
customer_flight_activity_df.printSchema()
customer_loyalty_history_df.printSchema()

"""# Data Cleaning"""

calendar_df.show(10)
customer_flight_activity_df.show(10)
customer_loyalty_history_df.show(10)

# Cek missing value masing-masing data
from pyspark.sql.functions import col, sum as _sum

def missing_count(df):
    return df.select([_sum(col(c).isNull().cast("int")).alias(c) for c in df.columns])

print("Missing values in calendar_df:")
missing_count(calendar_df).show()

print("Missing values in customer_flight_activity_df:")
missing_count(customer_flight_activity_df).show()

print("Missing values in customer_loyalty_history_df:")
missing_count(customer_loyalty_history_df).show()

"""## Penanganan data pada tabel customer_flight_activity"""

# memasukkan nilai 0 ke kolom point_accumulated yang bernilai Null
customer_flight_activity_df = customer_flight_activity_df.fillna({'points_accumulated': 0})

"""## Penanganan data pada tabel customer_loyalty_history"""

# cek nilai yang tidak Null pada kolom customer_lifetime_value
from pyspark.sql.functions import col

customer_loyalty_history_df.filter(col("customer_lifetime_value").isNotNull()).show(10)

# crosscek lagi nilai yang tidak Null pada kolom customer_lifetime_value
valid_count = customer_loyalty_history_df.filter(col("customer_lifetime_value").isNotNull()).count()
print("Jumlah baris customer_lifetime_value yang tidak NULL:", valid_count)

"""Kolom customer_lifetime_value tidak memiliki satu pun nilai valid (semua NULL), sehingga tidak digunakan dalam analisis lanjutan."""

# Kolom customer_lifetime_value dihapus
customer_loyalty_history_df = customer_loyalty_history_df.drop("customer_lifetime_value")

# cek nilai yang tidak Null pada kolom enrollment_year
customer_loyalty_history_df.filter(col("enrollment_year").isNotNull()).show(10)
valid_count = customer_loyalty_history_df.filter(col("enrollment_year").isNotNull()).count()
print("Jumlah baris enrollment_year yang tidak NULL:", valid_count)

# Kolom enrollment_year dihapus
customer_loyalty_history_df = customer_loyalty_history_df.drop("enrollment_year")

# cek nilai yang tidak Null pada kolom enrollment_month
customer_loyalty_history_df.filter(col("enrollment_month").isNotNull()).show(10)
valid_count = customer_loyalty_history_df.filter(col("enrollment_month").isNotNull()).count()
print("Jumlah baris enrollment_month yang tidak NULL:", valid_count)

# Kolom enrollment_month dihapus
customer_loyalty_history_df = customer_loyalty_history_df.drop("enrollment_month")

customer_loyalty_history_df.filter(
    col("cancellation_year").isNotNull() & col("cancellation_month").isNotNull()
).show()

"""Kolom cancellation_year dan cancellation_month tidak memiliki baris yang lengkap (tidak ada baris dengan kedua kolom isNotNull), sehingga **pada kasus ini** tidak digunakan untuk analisis lanjutan"""

# menginterpertasikan kolom sallary
null_salary_count = customer_loyalty_history_df.filter(col("salary").isNull()).count()
total_count = customer_loyalty_history_df.count()
percent_null_salary = (null_salary_count / total_count) * 100 if total_count else 0

print(f"Jumlah salary yang NULL: {null_salary_count}")
print(f"Jumlah total baris: {total_count}")
print(f"Persentase salary yang NULL: {percent_null_salary:.2f}%")

# cek data pada kolom salary
customer_loyalty_history_df.describe(['salary']).show()

# mencari baris dengan salary negatif
customer_loyalty_history_df.filter(col("salary") < 0).show()

# menghapus salary value yang bernilai negatif
customer_loyalty_history_df = customer_loyalty_history_df.filter(col("salary") >= 0)

"""Baris dengan salary negatif dihapus untuk menjaga validitas distribusi salary."""

# menghitung nilai median pada kolom salary untuk mengisi data Null
salary_median = customer_loyalty_history_df.approxQuantile("salary", [0.5], 0)[0]
print(salary_median)

# memasukkan median ke data Null
customer_loyalty_history_df = customer_loyalty_history_df.fillna({'salary': salary_median})

""" Nilai salary yang missing diimputasi dengan median untuk menjaga distribusi tetap representatif."""

# Drop dupplicate tiap tabel
calendar_df = calendar_df.dropDuplicates()
customer_flight_activity_df = customer_flight_activity_df.dropDuplicates()
customer_loyalty_history_df = customer_loyalty_history_df.dropDuplicates()

# Crosscek lagi
calendar_df.show(5)
customer_flight_activity_df.show(5)
customer_loyalty_history_df.show(5)

# cek potensial outlier
customer_flight_activity_df.filter(col("total_flights") >= 29).show()

"""## Simpan hasil analisa"""

# format csv
calendar_df.write.mode("overwrite").option("header", "true").csv("/content/hasil_analisis/clean_data/csv/calendar_cleaned.csv")
customer_flight_activity_df.write.mode("overwrite").option("header", "true").csv("/content/hasil_analisis/clean_data/csv/customer_flight_activity_cleaned.csv")
customer_loyalty_history_df.write.mode("overwrite").option("header", "true").csv("/content/hasil_analisis/clean_data/csv/customer_loyalty_history_cleaned.csv")

"""Mengubah nama file dan menyimpan hanya satu file csv"""

import glob

# Mencari file part-*.csv dari hasil Spark
calendar_part = glob.glob("/content/hasil_analisis/clean_data/csv/calendar_cleaned.csv/part-*.csv")[0]
flight_part = glob.glob("/content/hasil_analisis/clean_data/csv/customer_flight_activity_cleaned.csv/part-*.csv")[0]
loyalty_part = glob.glob("/content/hasil_analisis/clean_data/csv/customer_loyalty_history_cleaned.csv/part-*.csv")[0]

import shutil

# Menentukan path file output yang diinginkan
calendar_csv = "/content/hasil_analisis/clean_data/csv/calendar_cleaned_download.csv"
flight_csv = "/content/hasil_analisis/clean_data/csv/customer_flight_activity_cleaned_download.csv"
loyalty_csv = "/content/hasil_analisis/clean_data/csv/customer_loyalty_history_cleaned_download.csv"

# Salin dan rename file part-xxxx.csv menjadi file .csv biasa
shutil.copy(calendar_part, calendar_csv)
shutil.copy(flight_part, flight_csv)
shutil.copy(loyalty_part, loyalty_csv)

"""# Transformasi Data

## Total penerbangan & points per pelanggan per tahun
"""

# join tabel
joined_df = customer_flight_activity_df.join(
    customer_loyalty_history_df, on='loyalty_number', how='inner'
)

from pyspark.sql.functions import sum as _sum, avg

agg_df = joined_df.groupBy("loyalty_number", "year").agg(
    _sum("total_flights").alias("yearly_flights"),
    _sum("distance").alias("yearly_distance"),
    _sum("points_accumulated").alias("total_points_accumulated"),
    _sum("points_redeemed").alias("total_points_redeemed")
)
agg_df.show(5)

# Rata-rata poin per flight
from pyspark.sql.functions import when
agg_df = agg_df.withColumn(
    "avg_points_per_flight",
    when(agg_df["yearly_flights"] == 0, 0)
    .otherwise(agg_df["total_points_accumulated"] / agg_df["yearly_flights"])
)
agg_df.show(5)

# Net loyalty points (poin bersih yang tersisa)
agg_df = agg_df.withColumn(
    "net_points",
    agg_df["total_points_accumulated"] - agg_df["total_points_redeemed"]
)
agg_df.show(5)

# Rata-rata jarak per flight
agg_df = agg_df.withColumn(
    "avg_distance_per_flight",
    when(agg_df["yearly_flights"] == 0, 0)
    .otherwise(agg_df["yearly_distance"] / agg_df["yearly_flights"])
)
agg_df.show(5)

customer_flight_activity_df.filter(
    col("points_accumulated") != col("distance")
).count()

"""Berdasarkan hasil eksplorasi, seluruh data penerbangan memiliki points_accumulated = distance. Maka, setiap insight terkait poin loyalitas pada analisis ini ekuivalen dengan insight pada jarak tempuh."""

# CEK Ulang missing, duplikat, dan printSchema setelah transformasi
agg_df.printSchema()
agg_df.select([(agg_df[c].isNull().cast("int").alias(c)) for c in agg_df.columns]).groupBy().sum().show()
print(agg_df.count(), "rows (should have no duplicate combinations loyalty_number, year)")

"""Meyimpan hasil data_transform"""

agg_df.write.mode("overwrite").parquet("/content/hasil_analisis/data_transform/penerbangan_points")

"""## Analisis Akumulasi Poin Berdasarkan Pendidikan"""

education_points = customer_flight_activity_df.join(
    customer_loyalty_history_df.select("loyalty_number", "education"),
    on="loyalty_number", how="left"
).groupBy("education").agg(
    avg("points_accumulated").alias("avg_points_accumulated")
).orderBy("avg_points_accumulated", ascending=False)
education_points.show()

"""## Rata-rata Jumlah Penerbangan per Kelompok Gender"""

gender_flight = customer_flight_activity_df.join(
    customer_loyalty_history_df.select("loyalty_number", "gender"),
    on="loyalty_number", how="left"
).groupBy("gender").agg(
    avg("total_flights").alias("avg_flights_per_gender")
)
gender_flight.show()

"""## Total Points Berdasarkan Kelompok Salary"""

# Buat kategori salary
customer_loyalty_history_df = customer_loyalty_history_df.withColumn(
    "salary_group",
    when(col("salary") < 50000, "Low")
    .when((col("salary") >= 50000) & (col("salary") < 70000), "Medium")
    .when(col("salary") >= 70000, "High")
    .otherwise("Unknown")
)
customer_loyalty_history_df.show()

salary_points = customer_flight_activity_df.join(
    customer_loyalty_history_df.select("loyalty_number", "salary_group"),
    on="loyalty_number", how="left"
).groupBy("salary_group").agg(
    avg("points_accumulated").alias("avg_points")
)
salary_points.show()

"""## Menggabungkan info demografi"""

agg_activity = customer_flight_activity_df.groupBy("loyalty_number").agg(
    _sum("total_flights").alias("total_flights"),
    _sum("points_accumulated").alias("total_points")
)
agg_activity.show()

demografi_summary = agg_activity.join(
    customer_loyalty_history_df.select(
        "loyalty_number", "gender", "education", "salary", "marital_status", "country"
    ),
    on="loyalty_number",
    how="left"
)
demografi_summary.show(5)

# menyimpan hasil
demografi_summary.write.mode("overwrite").parquet("/content/hasil_analisis/data_transform/demografi_summary.parquet")

"""# Analisis SQL"""

# Register ke SQL Table:
joined_df.createOrReplaceTempView("joined_data")

# Rata-rata jumlah penerbangan per pelanggan per tahun
yearly_flight = """
SELECT loyalty_number, AVG(total_flights) as avg_flights_per_year
FROM joined_data
GROUP BY loyalty_number
"""
spark.sql(yearly_flight).show(10)

# Distribusi loyalty points berdasarkan loyalty_card
loyalty_points = """
SELECT loyalty_card, AVG(points_accumulated) as avg_points
FROM joined_data
GROUP BY loyalty_card
"""
spark.sql(loyalty_points).show()

# Hubungan pendidikan dengan jumlah penerbangan
education_flights = """
SELECT education, AVG(total_flights) as avg_flights
FROM joined_data
GROUP BY education
ORDER BY avg_flights DESC
"""
spark.sql(education_flights).show()

# Tren penerbangan waktu ke waktu
flight_trend = """
SELECT year, SUM(total_flights) as total_flights
FROM joined_data
GROUP BY year
ORDER BY year
"""
spark.sql(flight_trend).show()

"""# Visualisasi"""

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10,6))
sns.barplot(data=spark.sql(yearly_flight).toPandas(), x="loyalty_number", y="avg_flights_per_year")
plt.title('Top 20 Pelanggan dengan Rata-rata Jumlah Penerbangan per Tahun Tertinggi')
plt.xlabel('Loyalty Number')
plt.ylabel('Rata-rata Flight per Tahun')
plt.tight_layout()
plt.show()

plt.figure(figsize=(10,6))
sns.barplot(data=spark.sql(loyalty_points).toPandas(), x="loyalty_card", y="avg_points", palette="Blues_d")
plt.title('Distribusi Loyalty Points Berdasarkan Status Kartu Loyalitas')
plt.xlabel('Status Kartu Loyalitas')
plt.ylabel('Rata-rata Poin')
plt.tight_layout()
plt.show()

# Visualisasi rata-rata jumlah penerbangan berdasarkan tingkat pendidikan
plt.figure(figsize=(10,6))
sns.barplot(data=spark.sql(education_flights).toPandas(), x="education", y="avg_flights")
plt.title('Rata-rata Jumlah Penerbangan Berdasarkan Tingkat Pendidikan')
plt.xlabel('Tingkat Pendidikan')
plt.ylabel('Rata-rata Penerbangan')
plt.show()

# Visualisasi tren jumlah penerbangan per tahun
plt.figure(figsize=(10,6))
sns.lineplot(data=spark.sql(flight_trend).toPandas(), x="year", y="total_flights")
plt.title('Tren Jumlah Penerbangan Per Tahun')
plt.xlabel('Tahun')
plt.ylabel('Jumlah Penerbangan')
plt.show()

# Menyimpan results analisis ke dalam format CSV
spark.sql(yearly_flight).write.mode("overwrite").option("header", "true").csv("/content/hasil_analisis/data_analisis_sql/avg_flights_per_year.csv")
spark.sql(loyalty_points).write.mode("overwrite").option("header", "true").csv("/content/hasil_analisis/data_analisis_sql/loyalty_points_distribution.csv")
spark.sql(education_flights).write.mode("overwrite").option("header", "true").csv("/content/hasil_analisis/data_analisis_sql/education_flights.csv")
spark.sql(flight_trend).write.mode("overwrite").option("header", "true").csv("/content/hasil_analisis/data_analisis_sql/flight_trend.csv")

"""Mendownload ZIP folder hasil analisis"""

import os
import shutil

# Buat folder parent
os.makedirs("/content/hasil_analisis/clean_data/parquet", exist_ok=True)
os.makedirs("/content/hasil_analisis/data_transform", exist_ok=True)
os.makedirs("/content/hasil_analisis/data_analisis_sql", exist_ok=True)

# Remove directories created by Spark saves
spark_output_dirs = [
    "/content/hasil_analisis/data_analisis_sql/avg_flights_per_year.csv",
    "/content/hasil_analisis/data_analisis_sql/loyalty_points_distribution.csv",
    "/content/hasil_analisis/data_analisis_sql/education_flights.csv",
    "/content/hasil_analisis/data_analisis_sql/flight_trend.csv"
]

for dir_path in spark_output_dirs:
    if os.path.exists(dir_path) and os.path.isdir(dir_path):
        shutil.rmtree(dir_path)

# Parquet
calendar_df.write.mode("overwrite").parquet("/content/hasil_analisis/clean_data/parquet/calendar_cleaned.parquet")
customer_flight_activity_df.write.mode("overwrite").parquet("/content/hasil_analisis/clean_data/parquet/customer_flight_activity_cleaned.parquet")
customer_loyalty_history_df.write.mode("overwrite").parquet("/content/hasil_analisis/clean_data/parquet/customer_loyalty_history_cleaned.parquet")
demografi_summary.write.mode("overwrite").parquet("/content/hasil_analisis/data_transform/demografi_summary.parquet")

# CSV - pandas (agar satu file per query)
spark.sql(yearly_flight).toPandas().to_csv("/content/hasil_analisis/data_analisis_sql/avg_flights_per_year.csv", index=False)
spark.sql(loyalty_points).toPandas().to_csv("/content/hasil_analisis/data_analisis_sql/loyalty_points_distribution.csv", index=False)
spark.sql(education_flights).toPandas().to_csv("/content/hasil_analisis/data_analisis_sql/education_flights.csv", index=False)
spark.sql(flight_trend).toPandas().to_csv("/content/hasil_analisis/data_analisis_sql/flight_trend.csv", index=False)

!zip -r hasil_analisis.zip /hasil_analisis

from google.colab import files
files.download('hasil_analisis.zip')